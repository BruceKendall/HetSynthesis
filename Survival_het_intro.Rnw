\documentclass[10pt]{article}
\title{Survival heterogeneity review}
\author{Gordon A. Fox and Bruce E. Kendall}
\date{\today}
\pagestyle{headings}
\usepackage{amsmath}
\usepackage{varioref}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{iwona}
%\usepackage[math]{iwona}
\usepackage[normalem]{ulem}
\usepackage{lmodern}
\usepackage{wrapfig}
%
\usepackage{verbatim}
\input{"bib-commands.tex"}
%\bibliography{Survival_het_intro.bib}

\begin{document}

\sloppy

\maketitle

<<set-knitr-options,include=FALSE,cache=FALSE>>=
library(knitr)
library(formatR)
opts_chunk$set(tidy=TRUE,fig.align='center',echo=TRUE,autodep=TRUE,comment=NA,cache=FALSE,message=FALSE,tidy.opts=list(width.cutoff=60))
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

#Suppress warning messages

defaultW <- getOption("warn")
options(warn = -1)
@

<<other_libraries, echo=FALSE>>=
    library(survival)
    library(ggplot2)
    library(ggsurvfit)
    library(ggpubr)
    library(xtable)
@

Individuals vary in their survival probabilities. Not only because of variation in age, size, or stage, but because of their histories of nutrition, parasites, or disease, their varied access to resources or exposure to toxicants, competition, and their varied positions in social hierarchies. There may also be genetic variation that contributes to varied survival propensities. Consequently, there is a rich store of biological questions to be revealed through the study of survival. But it also presents some knotty conceptual and technical issues. We introduce these problems first, and then review results from published empirical studies.

\section{Estimating survival and its problems}

A key problem is that the survival probability of an individual is not measurable. We can measure its death date, but an early death may be due to bad luck, not an inherently poor survival probability. We can only estimate survival probabilities for aggregates of individuals. Doing so requires modeling, based on biological insight.

The models used for studying survival have some special characteristics. Survival processes generally do not lead to symmetric distributions of survival time, as they have an abrupt beginning, such as birth or the start of an experiment. Consider the three types of survival curves in \citet{Deevey1947}. If there are many late or early deaths (types I or III), obviously the survival times are not distributed normally. If mortality is constant (type I), survival times are distributed exponentially. As a consequence, survival models do not rely on the normal distribution, and the population mean (or marginal) survival is not generally a good descriptor for the entire population. In fact, the mean or marginal survival estimate may not describe survival for any individual. If there is substantial heterogeneity, these estimates may be far off, and the analysis may miss some important biology. Moreover, most data sets involve some censorship (time of death known only as an inequality, e.g., because it was after the end of the study, between two observations, or caused by the researcher. Because of the censorship and the skewed distribution of time-to-death, models based on the normal distribution are not generally used in survival analysis. 

Given this, how can we use models to estimate survival? There are many useful approaches. The most widely used are proportional hazard   models (including the Cox proportional hazards model and parametric hazard models), accelerated failure time models (AFT), and mark-recapture estimates \citep{Therneau2000, Kalbfleisch2002, Lebreton1992}. There are many forms of each. They have their own pros and cons in any application, and these are discussed elsewhere \citep{Fox2001a} \textbf(XX, bunch more cites). Here we focus on the issues (and pros and cons) related to heterogeneity in populations.

\subsection{Regression models}

The AFT model, in its simplest form, is 
\begin{equation*}
    \log T_{i} = x_{i}^{'} \beta + \sigma W_{i}
\end{equation*}
where $T_i$ is the event time for individual $i$, $x_{i}$ is a matrix of covariates and $x_{i}^{'}$ its transpose, $\beta$ is a vector of regressors, $\sigma$ is a dispersion factor, and $W$ is the error, typically distributed as Weibull or log-logistic. Less commonly some other distributions are used, including the gamma, Gompertz, and lognormal. 

The Cox PH model is
\begin{equation*}\label{eq:CoxPH}
    h_{i}(t) = h_{0}(t) \exp \left[\Sigma_{j=1}^{r} \beta_{j} X_{ij}\right]
\end{equation*}
where $h_{i}(t)$ is the hazard for individual $i$ at time $t$, $h_{0}$ is the baseline hazard, and the terms in the summation are covariates and regression terms, as in the AFT model. 

The terms in these two models differ in sign --- a positive effect of a covariate in the AFT model delays death, while the analogous term in a Cox PH model increases the hazard and thus tends to advance death. 

Both models have been extended to allow for time-varying covariates. The \texttt{survival} library in R can fit time-varying Cox PH models \citep{Therneau2000}, and the \texttt{eha} and \texttt{flexsurv} libraries can fit time-varying AFT models. Fitting time-varying models typically requires careful attention to reformatting of datasets.


\subsection{Identifiability} 
Individuals die once. It may seem intuitively impossible to estimate any sort of individual propensity for mortality. But it turns out that all that is needed are (a) a named parametric distribution of these propensities, the expected value of which is finite, and (b) some covariates in the model (\citep{Yashin2001, Elbers1982, Heckman1984, Balan2020}). For example, in a Cox proportional hazards (PH) model, one cannot estimate individual effects without specifying their parametric distribution, and some covariates. 

\subsection{Covariates}
Covariates are useful in survival models for a variety reasons, including these: 
\begin{itemize}
  \item Because sometimes that's where a lot of biological interest lies. E.g., we may want to know the effect of predator density on prey survival. Sometimes these have been manipulated experimentally.
  \item Because these factors induce variation in survival within the population.
  \item Because otherwise we may delude ourselves by e.g., just taking the mean survival. 
\end{itemize}

How is it that we may be deluding ourselves by ignoring underlying variation? Consider a simple example: there are two site types in a population. Individuals in both have constant survival probabilities, but in poor microsites, they survives each interval at $p \approx 0.86$, while the better sites have $p \approx 0.96$. The simulated survival process is shown in Figure \vref{fig:SurvHetPop}.
    
<<simulation1, echo=FALSE>>=
    source("SimpleSurvHetSimulation.R")
@
\begin{wrapfigure}{r}{0.5\textwidth}
<<survhetpop, fig=TRUE, echo=FALSE>>=
    ggarrange(gp.f0, gp.f1, ncol=2, nrow=1, align="hv")
@
\caption{Survival in a population composed of two types, one of which has superior survival. Survival estimated for the entire population, ignoring the heterogeneity (left), yields an estimate that does not represent that of any individual.}\label{fig:SurvHetPop}
\end{wrapfigure}

This example (Figure \vref{fig:SurvHetPop}) may seem extreme, but it provides several important conclusions. Most important, the cumulative survival probability for the pooled population does not reflect that for any individual. Moreover, reliance simply on the survival estimate for the pooled population would miss the biological processes underlying the difference. Ignoring the predictors (in this case, site) can lead to misestimation. For example, if the initial proportions of the two types were different (in Figure \vref{fig:SurvHetPop}), the right-hand figure would be unchanged, but the left-hand figure (for the pooled population) would be different.

\subsection{Omitted covariates} 
We often have data on some important predictors but not others. For example, we may have data about individual types of microsites, but we rarely have data about variation in gene loci that may affect survival. In ordinary least-squares regression with independent regressors, omitting covariates still allows for consistent estimates of the remaining covariates. This is not always true for survival models. Under the Cox PH model, omitting a covariate causes underestimation of the magnitude of remaining covariates \citep{Bretagnolle1988a}. Indeed, this appears to have been the cause of the so-called ``obesity paradox'' in health research, in which some scientists concluded erroneously that obesity has a protective effect on those with some problems like cardiovascular diseases. The intuitive explanation is simple \citep{Balan2020}: with two independent factors that both increase risk, those most at risk are those with large values of both. They tend to die early. Under a model with only one of these factors, over time a spurious negative correlation develops between the factors. The overall effect that is estimated is consequently too small for the included factor. AFT models are fairly insensitive to  omitted covariates. \citep{Gail1984, Struthers1986}. 

<<simul.frail, cache=TRUE, echo=FALSE>>=
    source("coxph_sim.R")
@

<<frail.tab, results='asis', echo=FALSE>>=
    frail.tab <- xtable(sum.df, auto=TRUE, caption="Summary of results of simulations including both covariates or only one.  Each
        of 200 runs used a sample population of 2000, with two independent covariates, each $\\sim \\mathcal{N}(0,1)$, with $\\beta_{1} = 1, \\beta_{2} = -1$, and an exponential baseline hazard. 50 time steps were used, with uniform censoring on $(20, 50)$. The table gives the mean and standard deviation of $\\hat{\\beta_{1}}$ for 200 runs of the simulation. We did not add a frailty term to the AFT models because the regressions were indistinguishable without them.", label="tab:frail.sum")

    rownames(frail.tab) <- c("Cox (1,2)", "Cox (1)", "Cox (1, f)", "AFT (1,2)", "AFT (1)")
    colnames(frail.tab) <- c("Mean($\\hat{\\beta_{1}}$)", "SD($\\hat{\\beta_{1}}$)")


    print(frail.tab, floating=TRUE, sanitize.text.function=function(x){x})   
@

Table \vref{tab:frail.sum} gives results of a simulation comparing the estimates of regression slopes under both Cox and accelerated failure time models, with and without a covariate, and with and without a frailty term. The same qualitative results occur if $\beta_{1} = \beta_{2}$. These results show that the AFT model is relatively robust to omitted covariates, but the Cox model is less so. Adding a frailty term can help with this problem in the Cox model. A minute's thought should make it clear why the Cox model is so sensitive to missing covariates. Since the hazard under this model has no specified form, without further information, an early (or late) death might be equally due to luck, or to individual heterogeneity. 


\subsection{Measurability} 
An important conceptual distinction is between measurable and nonmeasurable predictors. We can measure, at least in principle, the height of an organism. On the other hand, while we can posit that there are covariates that make some individuals more or less prone to die (these may be, e.g., genetic, site-related, or socially induced), in practice it may be often be impossible to measure some of these. Similarly, while we may find that some families live longer than others, characterizing the genetic and environmental factors that cause this is typically beyond our grasp in practice. Some of these unmeasurable effects can be absorbed into frailty terms. 


\subsection{Frailty} 
Including a random term -- usually called ''frailty'' -- can overcome some of the problem caused by omitted covariates. Frailty terms are random effects; in some respects they bear a similarity to random effects in the familiar generalized linear models. The term ``frailty model'' is used in at least two ways in the literature: it may refer to the form of the frailty term within a statistical model, or to the entire statistical model itself.

The meaning of ``random'' here is important. It does not imply that the factor occurs in some truly random sense, nor that it is sampled at random. This is similar to random effects in a GLMM, although some papers still repeat these fallacies. Different authors use the term ``frailty model'' differently. Some refer to the form the frailty term takes within the statistical model, while others use ``frailty model'' to mean the entire statistical model that includes a frailty term.  

If a frailty term is used, the AFT model becomes 
\begin{equation*}
    \log T_{i} = x_{i}^{'} \beta + \epsilon_{i} + \sigma W_{i}
\end{equation*}
where $\epsilon$ is the frailty term; because it is additive in this model for the log of time-to-event, it acts multiplicatively on the untransformed time. With the addition of frailty, the Cox PH model becomes 
\begin{equation*}\label{eq:CoxPHfrail}
    h_{i}(t) = Z_{i} h_{0}(t) \exp \left[\Sigma_{j=1}^{r} \beta_{j} X_{ij}\right]
\end{equation*}
where the $Z$ is the frailty. Thus the AFT and Cox PH models are on different scales, and they model, respectively, the log time to death and the hazard (the instantaneous risk of death). The terms differ in sign --- a positive effect of a covariate in the AFT model delays death, while the analogous term in a Cox PH model increases the hazard and thus tends to advance death. 

Frailty terms are most often taken to be gamma-distributed (typically with mean 0), because this turns out to be mathematically convenient. Other distributions are used, including the inverse gaussian (Wald) and positive stable distributions. There has been little study of how to choose the frailty distribution; \citet{Balan2020} suggest that the choice is typically made by convenience.

Under the \textbf{shared frailty} model \citep{Balan2020, Hougaard2000}, the frailty term is the same for all members of a cluster --- for example, a family or a site. The frailty term then represents the portion of frailty that is shared (say, by family members); frailty at other hierarchical levels is taken up by the underlying hazard function. The \textbf{correlated frailty} model \citep{Yashin1995a} relaxes the assumption that the frailty is identical for all members of a cluster, and can be estimated using the \texttt{frailtypack} library in R \citep{Rondeau2012}. There are numerous other types of frailty models, including time-varying \citep{Unkel2014} models. However, there is currently no standard software that can estimate these models.

Heterogeneity may occur at many different hierarchical levels (see Section X), and we expect that it is often likely to occur simultaneously at multiple levels. While this may be the case, it is not always practical to estimate such models. Sample size, of course, may be limiting, but there is a thornier problem: at present, there is little software that can estimate random effects for survival models at multiple levels. The \texttt{coxme} library in R does allow for mixed effects for the Cox PH model, and the \texttt{frailtyHL} library can fit models with multiple random levels for both Cox and accelerated failure time models. Models including shared frailty within specified groups can be estimated using the \texttt{streg} function in the Stan statistical modeling system. As of this writing, there is little experience with these approaches in ecology or evolutionary biology. Larger compendia of software for frailty modeling are provided in \citep{Balan2020} for PH models, and by \citep{Gorfine2023} for accelerated failure time models. 

Adding a frailty term to a model does not, unfortunately, provide an unambiguous estimate of the ``amount of heterogeneity'' in survival within a population. The estimate of the variance of the random effect depends, to varying degrees, on the fixed terms in the model, on the parametric form of the error terms (except in the Cox PH model), on the parametric form of the frailty, and on the adequacy of the model itself in describing the survival process. But this is similar to the situation with more familiar models, such as GLMMs: the values of quantities like variance components depend on the sampling scheme, on the validity of assumptions about normality, and on model adequacy. GLMMs, of course, permit more complex random structures than survival models currently do, but sample designs often limit the complexity of GLMMs that can be built for ecological problems. We argue that frailty models are an extremely valuable tool for population studies, and one that deserves more general use. 

\subsection{Discrete time}
What about estimates for survival in discrete time? This issue arises in several situations:
\begin{itemize}
  \item Censuses occurred at long intervals, so data are truly discrete.
  \item You wish to use estimates in some discrete-time application like an integral projection model (IPM) or matrix projection model (MPM). 
\end{itemize}

If data are truly discrete, then discrete-time survival models obviously make sense. There are a number of them; \citet{Tutz2016} provides a thorough overview. Perhaps the most widely used in ecological applications is the complementary log-log regression model, in which one fits a linear regression model for the transformed dependent variable $\log \left[-\log \left(1-P_{it} \right) \right]$, where $P_{it}$ is the empirical probability of death for the $i$-th individual in the $t$-th time interval. This model has a number of commonly used names, including the Gompertz model, the grouped proportional hazard model, the gompit model, and extreme value regression. It arises from discretizing the Cox proportional hazard model \citep{Tutz2016}. This linear model with a cloglog link can be fit using any software for glm models; more appropriately, individual frailties can be accounted for by fitting a glmm with a random term for individual. Moreover, right censorship can be handled appropriately because individuals are not considered after they die or are censored. 

There are several reasons to (usually) use discrete-time survival models rather than the more familiar logistic regression. Some authorities  recommend logistic regression by default (e.g.,\citet{Ellner2016})), but we argue that this is wrong. \textbf{Check if Caswell does this in his book too.} First, logistic regression leads to a survival estimate at the end of a study interval, while survival models lead to an estimated survival function throughout the study. Thus, using logistic regression typically leads to losing information. In particular, if there are covariates in the model, it is possible for survival curves to cross. Using logistic regression, one would miss this entirely. Second, survival models can use censored data, while logistic regression cannot. Again, this leads to a loss of information and potential distortion of results. The only case where logistic regression doesn't suffer by comparison to survival models is when there is no censorship, no covariates, and the organisms were censused only at the beginning and end of the study; then the two approaches should give similar results. One can easily calculate the cumulative probability of death in an interval from a survival curve.

\section{Todo}

\begin{itemize}
\bf{
             
    \item Mark-recapture data 
    
    \item Perhaps incorporate passages below. Or delete.
    
    \item Empirical
}
\end{itemize}





\section{Notes made earlier, to be incorporated or tossed}

One difficulty is that there are relatively few ecologists or evolutionists who are deeply familiar with the statistical methods used for analyzing survival data. Deep assumption in much of ecol/evol that GLM and its relatives covers most of the statistics needed. Most of the survival literature is in biostatistics and human demography; these are approachable, but the problems and the jargon are somewhat different.


This said, distinguishing between individual variation and stochasticity can be a knotty problem, especially because it is not always clear what is meant by stochasticity. Some individuals die quickly: did they have poor survival probabilities, or bad luck? In individual cases, we generally can't say, but appropriate sampling and modeling can address this problem. 


\printbibliography


\end{document}
