\documentclass[10pt]{article}
\title{Survival heterogeneity review}
\author{Gordon A. Fox and Bruce E. Kendall}
\date{\today}
\pagestyle{headings}
\usepackage{amsmath}
\usepackage{varioref}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{iwona}
%\usepackage[math]{iwona}
\usepackage[normalem]{ulem}
\usepackage{lmodern}
\usepackage{wrapfig}
%
\usepackage{verbatim}
\input{"bib-commands.tex"}
%\bibliography{Survival_het_intro.bib}

\begin{document}

\sloppy

\maketitle

<<set-knitr-options,include=FALSE,cache=FALSE>>=
library(knitr)
library(formatR)
opts_chunk$set(tidy=TRUE,fig.align='center',echo=TRUE,autodep=TRUE,comment=NA,cache=FALSE,message=FALSE,tidy.opts=list(width.cutoff=60))
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

#Suppress warning messages

defaultW <- getOption("warn")
options(warn = -1)
@

<<other_libraries, echo=FALSE>>=
    library(survival)
    library(ggplot2)
    library(ggsurvfit)
    library(ggpubr)
    library(xtable)
@

This is a working draft. Hell, it's not even that yet \ldots 
\section{Initial questions}
\begin{itemize}
    \item Why models?
    \item Why covariates?
    \item But there are many possible covariates, most of which can't be measured in practice.
    \item What if you ignore some, or just don't have enough data? This is one reason for using frailties
    \item What are some examples of usefully analyzed data? And what do these show?
\end{itemize}

\section{Introduction}

Individuals vary in their survival probabilities. Not only because of variation in age, size, or stage, but because of their histories of nutrition, parasites, or disease, their varied access to resources or exposure to toxicants, competition, and their varied positions in social hierarchies. There may also be genetic variation that contributes to varied survival propensities. Consequently, there is a rich store of biological questions to be revealed through the study of survival. But it also presents some knotty conceptual and technical issues. We introduce these problems first, and then review results from published empirical studies.

\section{Estimating survival and its problems}

A key problem is that the survival probability of an individual is not measurable. We can measure its death date, but an early death may be due to bad luck, not an inherently poor survival probability. We can only estimate survival probabilities for aggregates of individuals. Doing so requires modeling, based on biological insight.

An additional problem is that survival processes generally do not lead to symmetric distributions of survival time. Intuition on this point can be had by considering the three types of survival curves in \citet{Deevey1947} . If there are many late or early deaths (types I or III), obviously the survival times are not distributed normally. If mortality is constant (type I), survival times are distributed exponentially. Add to this the facts that survival processes have an abrupt beginning, such as birth or the start of an experiment, and that most data sets involve some censorship (time of death known only as an inequality, e.g., because it was after the end of the study, between two observations, or caused by the researcher. As a consequence, the population mean (or marginal) survival is not generally a good descriptor for the entire population. The mean or marginal survival estimate may not describe survival for any individual. If there is substantial heterogeneity, these estimates may be far off, and the analysis may miss some important biology.

Given this, how can we use models to estimate survival? There are many useful approaches. The most widely used are proportional hazard models (including the Cox proportional hazards model and parametric hazard models), accelerated failure time models (AFT), and mark-recapture estimates. There are many forms of each. They have their own pros and cons in any application, and these are discussed elsewhere (XX, bunch of cites). Here we focus on issues (and pros and cons) related to heterogeneity \textbf{rewrite}:
\begin{itemize}
    \item \textbf{Identifiability}. Individuals die once. It may seem intuitively impossible to estimate any sort of individual propensity for mortality. But it turns out that all that is needed are (a) a named parametric distribution of these propensities, the expected value of which is finite, and (b) some covariates in the model (\citep{Yashin2001, Elbers1982, Heckman1984, Balan2020}). For example, in a Cox proportional hazards model, one cannot estimate individual effects without specifying their parametric distribution, and some covariates. 
  
    \item \textbf{Omitted covariates}. We may have data on some important predictors but not others. For example, we may have data about individual types of microsites, but we rarely have data about variation in gene loci that may affect survival. In ordinary least-squares regression with independent regressors, omitting covariates still allows for consistent estimates of the remaining covariates. This is not always true for survival models. Under a proportional hazard model, omitting a covariate causes underestimation of the magnitude of remaining covariates \citep{Bretagnolle1988a}. Indeed, this appears to have been the cause of the so-called ``obesity paradox'' in health research, in which some scientists concluded erroneously that obesity has a protective effect on those with some problems like cardiovascular diseases. The intuitive explanation is simple \citep{Balan2020}: with two independent factors that both increase risk, those most at risk are those with large values of both. They tend to die early. Under a model with only one of these factors, over time a spuriousa negative correlation develops between the factors. The overall effect that is estimated is consequently too small for the included factor. At least for some distributions, omitted covariates do not have as large effects on AFT models as for the Cox proportional hazards model \citep{Gail1984, Struthers1986}. 
        
        Table \vref{tab:frail.sum} gives results of a simulation comparing the estimates of regression slopes under both Cox and accelerated failure time models, with and without a covariate, and with and without a frailty term. The same qualitative results occur if $\beta_{1} = \beta_{2}$. These results show that the AFT model is robust to omitted covariates, but the Cox model is less so. Adding a frailty term can help with this problem in the Cox model. 

    \item \textbf{Frailty}. Including a random term -- usually called ''frailty'' -- can overcome some of the problem caused by omitted covariates. Frailty terms are random effects; in some respects they bear a similarity to random effects in the familiar generalized linear models. The term ``frailty model'' is used in at least two ways in the literature: it may refer to the form of the frailty term within a statistical model, or to the entire statistical model itself.
        
    \item \textbf{Measurability}. An important conceptual distinction is between measurable and nonmeasurable predictors. We can measure, at least in principle, the height of an organism. On the other hand, while we can posit that there are predictors that make some individuals more or less prone to die (these may be, e.g., genetic, site-related, or socially induced), in practice it may be often be impossible to measure these. Similarly, while we may find that some families live longer than others, characterizing the genetic and environmental factors that cause this is typically beyond our grasp in practice. Some of these unmeasurable effects can be absorbed into frailty terms.

    \item \textbf{Levels of random variation}. Heterogeneity may occur at many different hierarchical levels (see Section X), and we expect that it is often likely to occur simultaneously at multiple levels. While this may be the case, it is not always practical to estimate such models. Sample size, of course, may be limiting, but there is a thornier problem: at present, there is little software that can estimate random effects for survival models at multiple levels. The \texttt{coxme} library in R does allow for mixed effects for the Cox proportional hazards model, and the \texttt{frailtyHL} library can fit models with multiple random levels for both Cox and accelerated failure time models. Models including shared frailty within specified groups can be estimated using the \texttt{streg} function in the Stan statistical modeling system. As of this writing, there is little experience with these approaches in ecology or evolutionary biology. Larger compendia of software for frailty modeling are provided in \citep{Balan2020} for proportional hazard models, and by \citep{Gorfine2023} for accelerated failure time models. 
        
\end{itemize}



<<simul.frail, cache=TRUE, echo=FALSE>>=
    source("coxph_sim.R")
@

<<frail.tab, results='asis', echo=FALSE>>=
    frail.tab <- xtable(sum.df, auto=TRUE, caption="Summary of results of simulations including both covariates or only one.  Each
        of 200 runs used a sample population of 2000, with two independent covariates, each $\\sim \\mathcal{N}(0,1)$, with $\\beta_{1} = 1, \\beta_{2} = -1$, and an exponential baseline hazard. 50 time steps were used, with uniform censoring on $(20, 50)$. The table gives the mean and standard deviation of $\\hat{\\beta_{1}}$ for 200 runs of the simulation. We did not add a frailty term to the AFT models because the regressions were indistinguishable without them.", label="tab:frail.sum")


    rownames(frail.tab) <- c("Cox (1,2)", "Cox (1)", "Cox (1, f)", "AFT (1,2)", "AFT (1)")
    colnames(frail.tab) <- c("Mean($\\hat{\\beta_{1}}$)", "SD($\\hat{\\beta_{1}}$)")


    print(frail.tab, floating=TRUE, sanitize.text.function=function(x){x})   
@




Frailty: what it is and isn't. Not really an estimate of heterogeneity, b/c of model dependence. 


There are many other types of survival models that may have potential uses in population biology. These include parametric hazard models




Some individuals die quickly: did they have poor survival probabilities, or bad luck? In individual cases, we generally can't say, but appropriate sampling and modeling can address this problem. Similarly, survival is usually affected by many things, some of which we can readily identify and measure, like family, site, or presence of a disease. Accounting for these is necessary, both to make appropriate estimates about the survival process, and to understand some of the causes underlying variation in survival.


\textbf{Mark-recapture data }




\subsection{Predictors in survival models}
There's a bestiary of names -- covariates, factors, random factors, independent variables, and so on. For the present, just call them all ``predictors.'' 
\begin{itemize}
  \item Because sometimes that's where a lot of biological interest lies. E.g., we may want to know the effect of predator density on prey survival. Sometimes these have been manipulated experimentally.
  \item Because these factors induce variation in survival within the population.
  \item Because otherwise we may delude ourselves by e.g., just taking the mean survival. 
\end{itemize}

How is it that we may be deluding ourselves by ignoring underlying variation? Consider a simple example: there are two types in a population. Both have constant survival probabilities, but one (perhaps in poor microsites) survives each interval at $p \approx 0.86$, while the better survivors have $p \approx 0.96$. The simulated survival process is shown in Figure \vref{fig:SurvHetPop}.
    
<<simulation1>>=
    source("SimpleSurvHetSimulation.R")
@
\begin{wrapfigure}{r}{0.5\textwidth}
<<survhetpop, fig=TRUE, echo=FALSE>>=
    ggarrange(gp.f0, gp.f1, ncol=2, nrow=1)
@
\caption{Survival in a population composed of two types, one of which has superior survival. Survival estimated for the entire population, ignoring the heterogeneity (left) does not represent that of any individual.}\label{fig:SurvHetPop}
\end{wrapfigure}

This example (Figure \vref{fig:SurvHetPop}) may seem extreme, but it provides several important conclusions. Most important, the cumulative survival probability for the pooled population does not reflect that for any individual. Moreover, reliance simply on the survival estimate for the pooled population would miss the biological processes underlying the difference. Ignoring the predictors (in this case, type) can lead to misestimation. For example, if the initial proportions of the two types were different (in this example they are equal), the right-hand figure would be unchanged, but the left-hand figure (for the pooled population) would be different.


An important conceptual distinction is between measurable and nonmeasurable predictors. We can measure, at least in principle, the height of an organism. On the other hand, while we can posit that there are predictors that make an individual more or less prone to some disease (these may be, e.g., genetic, site-related, or socially induced), in practice it may be often be impossible to measure these. Similarly, while we may find that some families live longer than others, characterizing the genetic and environmental factors that cause this is typically beyond our grasp in practice.


\section{Frailty}

\section{Notes made earlier, to be incorporated or tossed}

By its nature, estimating survival probabilities, and heterogeneity in them, presents some conceptual and technical challenges. Individuals have single sample - there's no replication. For individuals, we can observe date of death, but not the survival probability. And the process is inherently stochastic. So for individuals without any covariates, we can't distinguish between a high probability of survival and good luck, or vice versa.

But there are ways of estimating meaningful quantities. They all involve some combination of model-based estimation, measurement of meaningful covariates, and aggregation of individuals.

There are 3 main types of models:
\begin{itemize}
	\item AFT
	\item Hazard-based regression
	\item Logistic regression
	\item Mark/recapture approaches
\end{itemize}
They each have strengths and weaknesses; for an introduction, see XXX. 

One difficulty is that there are relatively few ecologists or evolutionists who are deeply familiar with the statistical methods used for analyzing survival data. Deep assumption in much of ecol/evol that GLM and its relatives covers most of the statistics needed. Most of the survival literature is in biostatistics and human demography; these are approachable, but the problems and the jargon are somewhat different.

Frailty models do not provide an estimate of the amount of heterogeneity, unless one can defend certain assumptions. This is because the variance of the random effect depends on the model specified and on the questions asked. CHECK ON THIS the random terms can be 
inflated/deflated by other terms in model. Obviously, its magnitude is meaningful in a qualitative sense. 

There are multiple frailty models, and multiple senses of the word. Individual frailty, shared frailty, correlated frailty are the most common. It seems likely there will be more. 

Frailty distibutions

Value of studying covariates which you *may* treat as random.

Study of heterogeneity in survival presents additional challenges. 

**********************************************************************
Individuals vary in their survival probabilities. Not only because of variation in age, size, or stage, but because of their histories of nutrition, parasites, or disease, their varied access to resources or exposure to toxicants, and their varied positions in social hierarchies. There may also be genetic variation that contributes to varied survival propensities. 

This said, distinguishing between individual variation and stochasticity can be a knotty problem. Some individuals die quickly: did they have poor survival probabilities, or bad luck? In individual cases, we generally can't say, but appropriate sampling and modeling can address this problem. 

There are several types of regression models used for survival data. These are somewhat specialized for survival data because survival times are not normally distributed, and the data are typically censored. By censored, we mean that we know only inequalities about the data (an individual survived at least as long as $\tau < x$, or between two values ($x_1 \leq \tau \leq x_2$), or less than some value ($\tau < x_2$). Censorship and non-normality usually make it impossible to use well-known approaches like GLM. Mark-recapture data 

But there are powerful methods for survival analysis, mainly developed in biostatistics and industrial reliability testing. 


Empirical studies: scrub-jays.

\printbibliography


\end{document}
