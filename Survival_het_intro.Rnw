\documentclass[10pt]{article}
\title{Survival heterogeneity review}
\author{Gordon A. Fox and Bruce E. Kendall}
\date{\today}
\pagestyle{headings}
\usepackage{amsmath}
\usepackage{varioref}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{iwona}
%\usepackage[math]{iwona}
\usepackage[normalem]{ulem}
\usepackage{lmodern}
%
\usepackage{verbatim}

\begin{document}

\sloppy

\maketitle

<<set-knitr-options,include=FALSE,cache=FALSE>>=
library(knitr)
library(formatR)
opts_chunk$set(tidy=TRUE,fig.align='center',echo=TRUE,autodep=TRUE,comment=NA,cache=FALSE,message=FALSE,tidy.opts=list(width.cutoff=60))
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

#Suppress warning messages

defaultW <- getOption("warn")
options(warn = -1)
@

<<other_libraries>>=
    library(survival)
    library(ggplot2)
    library(ggsurvfit)
    library(ggpubr)
@

This is a working draft. Hell, it's not even that yet \ldots TEST
\section{Initial questions}
\begin{itemize}
    \item Why models?
    \item Why covariates?
    \item But there are many possible covariates, most of which can't be measured in practice.
    \item What if you ignore some, or just don't have enough data? This is one reason for using frailties
\end{itemize}

\section{Introduction}

Individuals vary in their survival probabilities. Not only because of variation in age, size, or stage, but because of their histories of nutrition, parasites, or disease, their varied access to resources or exposure to toxicants, and their varied positions in social hierarchies. There may also be genetic variation that contributes to varied survival propensities. 

This said, distinguishing between individual variation and stochasticity can be a knotty problem. Some individuals die quickly: did they have poor survival probabilities, or bad luck? In individual cases, we generally can't say, but appropriate sampling and modeling can address this problem. Similarly, survival is usually affected by many things, some of which we can readily identify and measure, like family, site, or presence of a disease. Accounting for these is necessary, both to make appropriate estimates about the survival process, and to understand some of the causes underlying variation in survival.

There are several types of regression models used for survival data. These are somewhat specialized for survival data because survival times are not normally distributed, and the data are typically censored. By censored, we mean that we know only inequalities about the data (an individual survival time $x$ is greater than some value ($x \geq \tau$), between two values ($\tau_1 \leq x \leq \tau_2$), or less than some value( $x \leq \tau_2$)) It is also possible for the time of death to be known, but not the time of ``birth.'' Censorship and non-normality generally make it impossible to use well-known approaches like GLM. 

\textbf{Mark-recapture data }

But there are powerful methods for survival analysis, mainly developed in biostatistics, industrial reliability testing, and sociology (where it is often called event history analysis). 


\subsection{Predictors in survival models}
There's a bestiary of names -- covariates, factors, random factors, independent variables, and so on. For the present, just call them all ``predictors.'' 
\begin{itemize}
  \item Because sometimes that's where a lot of biological interest lies. E.g., we may want to know the effect of predator density on prey survival. Sometimes these have been manipulated experimentally.
  \item Because these factors induce variation in survival within the population.
  \item Because otherwise we may delude ourselves by e.g., just taking the mean survival. 
\end{itemize}

How is it that we may be deluding ourselves by ignoring underlying variation? Consider a simple example: there are two types in a population. Both have constant survival probabilities, but one (perhaps in poor microsites) survives each interval at $p \approx 0.86$, while the better survivors have $p \approx 0.96$. The simulated survival process is shown in Figure \vref{fig:SurvHetPop}.

<<simulation1>>=
    source("SimpleSurvHetSimulation.R")
@

\begin{center}
\begin{figure}
<<survhetpop, fig=TRUE, echo=FALSE>>=
    ggarrange(gp.f0, gp.f1, ncol=2, nrow=1, common.legend=TRUE)
@
\caption{Survival in a population composed of two types, one of which has superior survival. Survival in the total population (left) does not represent that of any individual.}\label{fig:SurvHetPop}
\end{figure}
\end{center}

This example (Figure \vref{fig:SurvHetPop}) may seem extreme, but it provides several important conclusions. Most important, the cumulative survival probability for the total population does not reflect that for any individual. A change in the initial composition of the population (here they were equal), for example, would give a different total survival function. Moreover, reliance simply on the survival estimate for the total population would miss the biological processes underlying the difference. Ignoring the predictors (in this case, type) can lead to misestimation.


An important conceptual distinction is between measurable and nonmeasurable predictors. We can measure, at least in principle, the height of an organism. On the other hand, while we can posit that there are predictors that make an individual more or less prone to some disease (these may be, e.g., genetic, site-related, or socially induced), in practice it may be often be impossible to measure these. Similarly, while we may find that some families live longer than others, characterizing the genetic and environmental factors that cause this is typically beyond our grasp in practice.

\clearpage

\section{Frailty}

\section{Notes made earlier, to be incorporated or tossed}

By its nature, estimating survival probabilities, and heterogeneity in them, presents some conceptual and technical challenges. Individuals have single sample - there's no replication. For individuals, we can observe date of death, but not the survival probability. And the process is inherently stochastic. So for individuals without any covariates, we can't distinguish between a high probability of survival and good luck, or vice versa.

But there are ways of estimating meaningful quantities. They all involve some combination of model-based estimation, measurement of meaningful covariates, and aggregation of individuals.

There are 3 main types of models:
\begin{itemize}
	\item AFT
	\item Hazard-based regression
	\item Logistic regression
	\item Mark/recapture approaches
\end{itemize}
They each have strengths and weaknesses; for an introduction, see XXX. 

One difficulty is that there are relatively few ecologists or evolutionists who are deeply familiar with the statistical methods used for analyzing survival data. Deep assumption in much of ecol/evol that GLM and its relatives covers most of the statistics needed. Most of the survival literature is in biostatistics and human demography; these are approachable, but the problems and the jargon are somewhat different.

Frailty models do not provide an estimate of the amount of heterogeneity, unless one can defend certain assumptions. This is because the variance of the random effect depends on the model specified and on the questions asked. CHECK ON THIS the random terms can be 
inflated/deflated by other terms in model. Obviously, its magnitude is meaningful in a qualitative sense. 

There are multiple frailty models, and multiple senses of the word. Individual frailty, shared frailty, correlated frailty are the most common. It seems likely there will be more. 

Frailty distibutions

Value of studying covariates which you *may* treat as random.

Study of heterogeneity in survival presents additional challenges. 

**********************************************************************
Individuals vary in their survival probabilities. Not only because of variation in age, size, or stage, but because of their histories of nutrition, parasites, or disease, their varied access to resources or exposure to toxicants, and their varied positions in social hierarchies. There may also be genetic variation that contributes to varied survival propensities. 

This said, distinguishing between individual variation and stochasticity can be a knotty problem. Some individuals die quickly: did they have poor survival probabilities, or bad luck? In individual cases, we generally can't say, but appropriate sampling and modeling can address this problem. 

There are several types of regression models used for survival data. These are somewhat specialized for survival data because survival times are not normally distributed, and the data are typically censored. By censored, we mean that we know only inequalities about the data (an individual survived at least as long as $\tau < x$, or between two values ($x_1 \leq \tau \leq x_2$), or less than some value ($\tau < x_2$). Censorship and non-normality usually make it impossible to use well-known approaches like GLM. Mark-recapture data 

But there are powerful methods for survival analysis, mainly developed in biostatistics and industrial reliability testing. 


Empirical studies: scrub-jays.


\end{document}
